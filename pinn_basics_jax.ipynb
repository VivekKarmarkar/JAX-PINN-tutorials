{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "204a15ed",
   "metadata": {},
   "source": [
    "# PINN basics in JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c27adb7",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ba8f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbcf821",
   "metadata": {},
   "source": [
    "## Note on packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962129da",
   "metadata": {},
   "source": [
    "1. JAX is supposed to be very fast owing to its features like just-in-time (jit) compilation and vectorized mapping of function (vmap). It is highly modular and has a steeper learning curve as compared to PyTorch\n",
    "2. JAX differs from PyTorch in terms of many options being available to build NNs (equinox and linen) and optimize (optax and flax) them. On the contrary, PyTorch is a single unified multi-purpose and user friendly platform\n",
    "3. Flax and Linen were designed to mimic the style of PyTorch\n",
    "4. Here, we will be using equinox (neural nets focused) and optax (optimization focused)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc4fe0b",
   "metadata": {},
   "source": [
    "## Problem definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68393d1",
   "metadata": {},
   "source": [
    "1D Poisson equation with homogeneous Dirichlet boundary conditions on the unit interval\n",
    "$$ \\\\ $$\n",
    "$$ \\frac{\\partial^2 u}{\\partial x^2} = -f(x), x \\in (0,1) $$\n",
    "$$ \\\\ $$\n",
    "$$ u(0) = u(1) = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6354ef",
   "metadata": {},
   "source": [
    "## Problem setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb5a86",
   "metadata": {},
   "source": [
    "By the \"observed\" universal approximation capabilities of the Neural Network (NN), we will approximate $u(x)$, the function that we would like to find with a NN parameterized by weights $\\theta$ as $\\hat{u}_{\\theta}$.\n",
    "$$ \\\\ $$\n",
    "Analogous to the form of the PDE, we can cook up a loss function term for a point labelled by $x_{i}$ in the domain as:\n",
    "$$ \\mathcal{L}_{i}(\\theta) = (\\frac{\\partial^2 \\hat{u}_{\\theta}}{\\partial x^2} + f(x))^2 \\vert_{x=x_{i}} $$\n",
    "$$ \\\\ $$\n",
    "The loss function part arising from the PDE can thus be written as:\n",
    "$$ \\mathcal{L}_{\\text{PDE}}(\\theta)  = \\sum_{i=0}^{L} \\mathcal{L}_{i}(\\theta)  = \\sum_{i=0}^{L} (\\frac{\\partial^2 \\hat{u}_{\\theta}}{\\partial x^2} + f(x))^2 \\vert_{x=x_{i}} $$\n",
    "$$ \\\\ $$\n",
    "We will also have a \"conventional\" loss function part arising from the data and since here only the boundary condition data is provided, this will be the boundary condition loss $\\mathcal{L}_{\\text{BC}}(\\theta) $. This part of the loss function is simply the \"vanilla\" sums of squares given by: $$ \\mathcal{L}_{\\text{BC}}(\\theta) = \\frac{1}{2}(\\hat{u}_{\\theta}(0) - u(0))^2 + \\frac{1}{2}(\\hat{u}_{\\theta}(1) - u(1))^2 $$\n",
    "$$ \\\\ $$\n",
    "The total loss $\\mathcal{L}(\\theta)$ with regularization $\\lambda_{BC}$ is given by:\n",
    "$$ \\mathcal{L}(\\theta) = \\mathcal{L}_{\\text{PDE}}(\\theta) + \\lambda_{BC} \\mathcal{L}_{\\text{BC}}(\\theta) $$\n",
    "$$ \\\\ $$\n",
    "The optimization problem is then given by:\n",
    "$$ \\theta^{*} =\\text{arg} \\min_{\\theta} \\mathcal{L}(\\theta) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f5b160",
   "metadata": {},
   "source": [
    "## Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dof_fd = 100\n",
    "L = 50\n",
    "learning_rate = 1e-3\n",
    "n_epochs = 5000\n",
    "bc_loss_weight = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56069e9e",
   "metadata": {},
   "source": [
    "## Generate mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_full = jnp.linspace(0.0, 1.0, n_dof_fd + 2)\n",
    "mesh_interior = mesh_full[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49453d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e396e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh_full.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539b6f4",
   "metadata": {},
   "source": [
    "## Define our function f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04904df",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs_function = lambda x: jnp.where((x > 0.3) & (x < 0.5), 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9777d",
   "metadata": {},
   "source": [
    "## Plot f(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68310fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(mesh_full[1:-1], rhs_function(mesh_full[1:-1]), label=\"Forcing function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903abadd",
   "metadata": {},
   "source": [
    "## Reproducibility using random key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49796e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf8d25",
   "metadata": {},
   "source": [
    "## Set up PINN - coordinate based NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf0b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, init_key = jr.split(key)\n",
    "pinn = eqx.nn.MLP(\n",
    "    in_size=\"scalar\",\n",
    "    out_size=\"scalar\",\n",
    "    width_size=10,\n",
    "    depth=4,\n",
    "    activation=jax.nn.sigmoid,\n",
    "    key = key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20effb33",
   "metadata": {},
   "source": [
    "## Notes on PINN setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e90f0",
   "metadata": {},
   "source": [
    "1. We are setting up a network that maps a scalar to a scalar\n",
    "2. The design choices of the network are arbitrary\n",
    "3. We are using a shallow network with 4 layers\n",
    "4. The number of neurons per layer is 10\n",
    "5. We are using the sigmoid activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fa3e98",
   "metadata": {},
   "source": [
    "## Generate initial prediction at x = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac6a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn(0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e4e984",
   "metadata": {},
   "source": [
    "## Apply PINN on mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d03d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinn(mesh_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ac6a9e",
   "metadata": {},
   "source": [
    "1. We get an error since the PINN was directed to take a scalar input\n",
    "2. This \"scalar\" issue can be fixed using jax's functionality called \"vmap\" that allows us to essentially take a function that is designed to act on scalars and apply it to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3fe33e",
   "metadata": {},
   "source": [
    "## Vectorized map functionality in jax - vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded1c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.vmap(pinn)(mesh_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf945aa6",
   "metadata": {},
   "source": [
    "## PDE residuum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40711f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_residuum(network, x):\n",
    "    return jax.grad(jax.grad(network))(x) + rhs_function(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad0ab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pde_residuum(pinn, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df0afba",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.vmap(pde_residuum, in_axes=(None, 0))(pinn, mesh_interior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e949d8",
   "metadata": {},
   "source": [
    "## Notes on vmap applied to PDE residuum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e08d31",
   "metadata": {},
   "source": [
    "1. The pde_residuum is a function that takes as input the pinn and a scalar x\n",
    "2. Since we want to evaluate pde_residuum over the mesh which is a vector we need use vmap\n",
    "3. Since we do not want to apply the vectorization to our pinn input, we set the first entry of in_axes to \"None\"\n",
    "4. Since we want to apply the vectorization to the the first axes of the scalar input x, we set the second entry to the index corresponding to the first axes which is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70be9777",
   "metadata": {},
   "source": [
    "## Total loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3aeeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(network):\n",
    "    pde_residuum_total = jax.vmap(pde_residuum, in_axes=(None, 0))(network, mesh_interior)\n",
    "    pde_loss_total = 0.5 * jnp.mean(jnp.square(pde_residuum_total))\n",
    "    bc_loss = 0.5 * jnp.square(network(0.0) - 0.0) + 0.5 * jnp.square(network(1.0) - 0.0)\n",
    "    loss_total = pde_loss_total + bc_loss_weight * bc_loss\n",
    "    return loss_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb0c6e5",
   "metadata": {},
   "source": [
    "## Check initial loss of PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f805aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(pinn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37049d47",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(eqx.filter(pinn, eqx.is_array))\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(network, optimizer_state):\n",
    "    loss, grad = eqx.filter_value_and_grad(loss_fn)(network)\n",
    "    network_updates, new_optimizer_state = optimizer.update(grad, optimizer_state, network)\n",
    "    new_network = eqx.apply_updates(network, network_updates)\n",
    "    return new_network, new_optimizer_state, loss\n",
    "\n",
    "start_time = time.time()\n",
    "loss_history = []\n",
    "for epoch in range(n_epochs):\n",
    "    pinn, opt_state, loss = make_step(pinn, opt_state)\n",
    "    loss_history.append(loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss}\")\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419b1f7-2a64-4733-96e4-e829b0eafb45",
   "metadata": {},
   "source": [
    "## Print Execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ecf124-a3b6-467e-9df5-784873e7bc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Execution time: %s seconds\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcac68f",
   "metadata": {},
   "source": [
    "## Notes on Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f21c70",
   "metadata": {},
   "source": [
    "1. We first create the ADAM optimizer with the learning rate specified in the hyperparameters section\n",
    "2. Next, the weights and biases of the pinn are initialized - equinox applies a fliter to extract the PINN parameters that are arrays and then these are initialized using the \"optimizer.init\" command\n",
    "3. The make step function performs a single optimization step\n",
    "4. In the make step function, we start with computing the loss and its gradient with respect to the \"trainable\" parameters of the network, again extracted via the \"filter\" part in \"filter_value_and_grad\"\n",
    "5. Next, we use the gradients to compute updates that need to be applied to the network (updated weights and biases). In addition this line of code also provides the new state of the ADAM optimizer (new running averages and squared values of the gradient)\n",
    "6. Finally, we apply the network update to the network (change weights and biases)\n",
    "7. The loop at the end of the section simply performs the optimization iteratively using make step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a5eb53",
   "metadata": {},
   "source": [
    "## Plot loss history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb2e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.yscale(\"log\")\n",
    "plt.title(\"loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0103bccf",
   "metadata": {},
   "source": [
    "## Function to compute reference solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reference_solution(mesh_full):\n",
    "    mesh_interior = mesh_full[1:-1]\n",
    "    rhs_evaluated = rhs_function(mesh_interior)\n",
    "    dx = mesh_interior[1] - mesh_interior[0]\n",
    "    A = jnp.diag(jnp.ones(n_dof_fd - 1), -1) + jnp.diag(jnp.ones(n_dof_fd - 1), 1) - jnp.diag(2*jnp.ones(n_dof_fd), 0)\n",
    "    A /= dx**2\n",
    "    finite_difference_solution = jnp.linalg.solve(A, -rhs_evaluated)\n",
    "    wrap_bc = lambda u: jnp.pad(u, (1, 1), mode=\"constant\")\n",
    "    reference_solution = wrap_bc(finite_difference_solution)\n",
    "    return reference_solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372d375b",
   "metadata": {},
   "source": [
    "## Plot solutions post network training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2636ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(mesh_full, compute_reference_solution(mesh_full), 'r', label=\"Reference solution\")\n",
    "plt.plot(mesh_full, jax.vmap(pinn)(mesh_full), 'b*', label='Final PINN solution')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"PDE solutions\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"u(x)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
