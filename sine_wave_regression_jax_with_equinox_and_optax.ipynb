{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef916802-3b40-4bcc-a337-46e81140ee95",
   "metadata": {},
   "source": [
    "# Sine wave regression in JAX with Equinox and Optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f3d40-c452-43fe-8359-8e87345a205d",
   "metadata": {},
   "source": [
    "## Import relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728b60a-fe67-4185-8ab4-e35a33484000",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869acfa3-9b27-4dc7-bc1c-72f4094082b3",
   "metadata": {},
   "source": [
    "## Note on importing List from the typing module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd738b8f-2e97-4bcf-95c4-7cd2c110623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_to_str_custom(original_list: list[int]):\n",
    "    converted_list = [str(int_iter) for int_iter in original_list]\n",
    "    return converted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43879c8-d970-4a93-8c7e-3db7e25d6a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_str_custom([1, 2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cdaf2e-1873-4478-bb85-5fe56da89431",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_str_custom([\"a\", \"b\", \"c\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df2c9f6-a020-46f8-b1d7-3ade211b3494",
   "metadata": {},
   "source": [
    "1. Importing List from the type module allows useage of type hints for lists where we can specify the type of the contents of the list\n",
    "2. For the Python version on my laptop it is not a strict requirement to adhere to the specified type as can be seen from the above example\n",
    "3. For the Python version on my laptop importing List from typing is also not required"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7765870-6e83-44f8-9f7e-df115f3566b1",
   "metadata": {},
   "source": [
    "## Set up hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cad42dd-9cf0-40ec-b732-6ffbacb9b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 200\n",
    "learning_rate = 0.1\n",
    "n_epochs = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f70e9b-0e21-498d-9eb6-e62894231f95",
   "metadata": {},
   "source": [
    "## Set up network layer sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea82cb83-298c-4db4-8895-060d3bca6dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input_neurons = 1\n",
    "#n_hidden_layers = 3\n",
    "n_hidden_layers = 2\n",
    "n_hidden_neurons = 10\n",
    "n_output_neurons = 1\n",
    "layers = [n_input_neurons] + n_hidden_layers*[n_hidden_neurons] + [n_output_neurons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca56b7e1-1f50-49bb-84c4-f6ff8fd5fac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6dbe1c-5a27-4bf8-bb6c-1fa16c5dbd91",
   "metadata": {},
   "source": [
    "## Generate toy data as column vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d682a8e-d2c8-40cd-98b8-c84ee55ba9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples = jnp.linspace(0, 2*jnp.pi, n_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6549286-a03e-4d72-8552-f78c2cc0ab3e",
   "metadata": {},
   "source": [
    "## Check shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07546f32-9082-4e89-a485-a076d03440ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea4260-230d-4775-b1c0-b75a28d13450",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ac9be-4ec3-4410-b883-7259cdbd0c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples = x_samples.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfe9d3-05e7-4877-bbbb-bc30a55fe465",
   "metadata": {},
   "source": [
    "## Check shape after reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300d0fd-8de9-4f6d-898c-cc99ce72cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c392916-c922-4024-a7c9-ec3c2be55a0e",
   "metadata": {},
   "source": [
    "## Generate sine wave column vector and check shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e821224-cbfa-434f-a2d9-212a224cc9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_samples = jnp.sin(x_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009da10-f016-4323-aefb-3269efa1899b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cdcf6b-2f5e-4d96-880d-cdb8d607b5bf",
   "metadata": {},
   "source": [
    "## Plot toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2559fdcf-9e91-49ad-b5a8-d08e35692867",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_samples, y_samples)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"toy data for regression\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea750242-75d0-477b-a0b7-be02c5c3db13",
   "metadata": {},
   "source": [
    "## Define a simple MLP class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885446dc-e0c3-43de-a1be-e707bb7856ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simpleMLP(eqx.Module):\n",
    "\n",
    "    layers: List[eqx.nn.Linear]\n",
    "\n",
    "    def __init__(self, layers_size_params, key):\n",
    "        self.layers = []\n",
    "        for (dim_in, dim_out) in zip(layers_size_params[:-1], layers_size_params[1:]):\n",
    "            key, subkey = jax.random.split(key)\n",
    "            self.layers.append(\n",
    "                eqx.nn.Linear(dim_in, dim_out, use_bias=True, key=subkey)\n",
    "            )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        a = x\n",
    "        for layer in self.layers[:-1]:\n",
    "            a = jax.nn.sigmoid(layer(a))\n",
    "        a = self.layers[-1](a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acd36c0-ceda-4c50-8cb2-6ac41aa82524",
   "metadata": {},
   "source": [
    "## Notes on simple MLP class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862d74a-8780-4b01-837d-9526ee260d7e",
   "metadata": {},
   "source": [
    "1. We should start by creating a skeleton of the class - that is we define the class name, the parent class it inherits from (in this case, equinox's Module) and the initialization (using init) and forward passes (using call)\n",
    "2. Next we should think about filling the initialization block - the purpose of this block is to initialize the Neural Network layers - for which we need the layer size parameters and a random key\n",
    "3. We start by defining an attribute \"layers\" that is initialized via assignment of an empty array\n",
    "4. Next, we need to fill this list of layers with randomly initialized weights and since the shapes of the weights are of the form (n_in, n_out) - we will need to simultaneously iterate over the list containing the input neuron sizes and the output neuron sizes. This simultaneous iteration can be achieved via the zip function. However, we will first need iterable containers that correspond to the varying number of input and output neurons at each layers - this can be easily obtained by noticing that the inputs terminate at the last layer meaning that the layers size parameter list needs to be truncated at the last index as \"layers_size_params[:-1]\" and that the outputs begin after the first layer meaning that the layers size parameters list needs to start after the first index as \"layers_size_params[1:]\"\n",
    "5. Now that we have got the dimensions for initializing weights and biases in place, we need a random key for each layer which can be obtained by splitting the input key resulting in two new keys - the first key will be used to overwrite the input key and the second key will be used for the weights initialization. The intended splitting operation is carried out via the jax.random.split(key) command\n",
    "6. Finally, we have all the ingredients in place to initialize all the layers - the dimensions and the random keys. We already defined an attribute called \"layers\" that was initialized via assignment of an empty list. This list was intended as a placeholder and it is now time to fill it!\n",
    "7. We fill this list via the classic \"append\" operation where we append entities that are of the type \"eqx.nn.Linear\" as specified in the type hint at the beginning of the simpleMLP class. We will thus use the eqx.nn.Linear command with the appropriate number of input and output neurons for each layer along with the random key. In addition, we will require the linear layers to have a bias term\n",
    "8. We end the class with a method to do the forward pass via the \"call\" keyword. Here, we define a variable \"a\" to represent the network output and initialize it with the input \"x\". Next, we iterate through the input and hidden layers via \"self.layers[:-1]\" and apply a forward pass by iteratively carrying out a non-linear sigmoidal transformation on the variable \"a\" and feeding it back to the loop. To conclude the forward pass, we simply apply the last layer as a linear transformation outside the loop and return the variable \"a\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24ddab-9e76-4f27-b417-6e7e055c1d69",
   "metadata": {},
   "source": [
    "## Maybe animate above code block with your ChatGPT manim idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774817bb-5fb4-4ef9-bb8d-3d6c6b39e2ad",
   "metadata": {},
   "source": [
    "## Initialize NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b40f8e6-0bbb-4eed-b005-86fef7f3b1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = simpleMLP(layers, key=jax.random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bad937-170c-4539-ad5b-ce20c45bc479",
   "metadata": {},
   "source": [
    "## Generate Initial prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7e4cc0-817e-4502-b217-feb03da6109d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_pred = model(x_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa86fa-8979-4f91-b7cd-02621e245201",
   "metadata": {},
   "source": [
    "## Notes on the error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec878ad-763d-4e5d-9062-6aba0e035000",
   "metadata": {},
   "source": [
    "1. The error is related to dimensions\n",
    "2. The documentation suggests using jax's vmap feature on the model to fix the error\n",
    "3. Using vmap does fix the error\n",
    "4. However it is not clear to me why vmap fixes the error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb492f-192d-4790-b73a-cfa57093181a",
   "metadata": {},
   "source": [
    "## Fix error with vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738443f9-6b35-48ff-b8be-6c5eab38056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_pred = jax.vmap(model)(x_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab32dda-4796-45b1-b864-85c92de9dbf6",
   "metadata": {},
   "source": [
    "## Plot initial prediction overlayed on toy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7280187-efd2-42dd-80b7-527ee0228030",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_samples, y_samples, label=\"toy data\")\n",
    "plt.plot(x_samples, initial_pred, 'red', label=\"initial prediction\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Plot intial prediction of NN\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027b513a-f835-4ab0-8a24-39b75840fa01",
   "metadata": {},
   "source": [
    "## Define MSE Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40885576-6132-4d00-af17-865996e9c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(model, x, y):\n",
    "    delta = y - jax.vmap(model)(x)\n",
    "    loss_val = jnp.mean(jnp.square(delta))\n",
    "    return loss_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f40944-2ad0-4cea-843c-851de4f22d81",
   "metadata": {},
   "source": [
    "## Compute Initial Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa17b08d-1aa0-49e7-9e77-57eaede49bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_loss = mse_loss(model, x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e4e52-049c-4fdd-8169-548d91e15376",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd972d-befa-428c-a42b-a9ed733bbeee",
   "metadata": {},
   "source": [
    "## Introducing Loss and grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c775da37-2ef2-4610-824c-075b5d3127e8",
   "metadata": {},
   "source": [
    "1. Jax and Equinox have functions that provide the function along with its gradients (value and grad)\n",
    "2. To be on the safe side, we would like to apply the grad with respect to parameters satisfying certain conditions\n",
    "3. The \"filter value and grad\" functionality accomodates for such \"filtering\" or \"selection\"\n",
    "4. The default filter used by filter value and grad is \"inexact arrays\" - where arrays stand for a collection of numbers and the simple explanation of inexact is floating point numbers\n",
    "5. For our use case the filter value and grad should compute the gradients with respect to parameters that are stored as arrays of floats - in other words, we are \"selecting\" or \"filtering\" the weights and biases of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932ac8cc-001a-4396-80fd-6c41b01a38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_and_grad = eqx.filter_value_and_grad(mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c20366-df43-4b98-a681-05fabd7d8fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_and_grad(model, x_samples, y_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455fb9e6-15ab-4d8e-aa3d-5dea24f1ac06",
   "metadata": {},
   "source": [
    "## Set up the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43b07c9-01ef-4d02-b1e6-7b6c9e6b3fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optax.sgd(learning_rate)\n",
    "opt_state = opt.init(eqx.filter(model, eqx.is_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583f0334-4c78-4164-b56e-594627539ae2",
   "metadata": {},
   "source": [
    "## Notes on optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32e165-bdb4-4487-9a70-e2d5561c275d",
   "metadata": {},
   "source": [
    "1. We will have initialized the SGD optimizer with the learning rate specificed in the hyperparameters section\n",
    "2. We then initialize the state of the optimizer with information about the weights and biases of the network\n",
    "3. To be careful with using the weights and biases, we apply equinox's filtering capabilities and filter for arrays\n",
    "4. It is worth noting that optax is the \"optimization engine\" that peforms \"gradient processing\" and the important information associated with the processing of these gradients required to execute the optimization algorithm at hand is stored in the state of the optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bbf5a0-4cfd-4d8e-b62d-0f8828a798c3",
   "metadata": {},
   "source": [
    "## Define the \"make step\" function to perform a single optimization step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2171c3-4bda-4937-ac13-3596696bfcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def make_step(model, opt_state, x, y):\n",
    "    loss, grad = mse_loss_and_grad(model, x, y)\n",
    "    model_update, opt_state = opt.update(grad, opt_state, model)\n",
    "    model = eqx.apply_updates(model, model_update)\n",
    "    return model, opt_state, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2050391f-37b4-40ea-97f6-baff25a7313f",
   "metadata": {},
   "source": [
    "## Notes on make step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed0b89e-8728-480c-a506-7b54b29fd367",
   "metadata": {},
   "source": [
    "1. The general neural network optimization workflow for a given step is as follows: extract gradients -> compute updates via optimization step -> apply update\n",
    "2. The mse_loss_and_grad function computes the loss and grad with the model, x and y as inputs\n",
    "3. The extracted gradients are used to perform the optimization step via \"opt.update\" and this produces a new optimizer state and updates that we could apply to the model (new weights and biases)\n",
    "4. The \"eqx.apply_updates\" line of code updates the weights and biases of the network to the new weights and biases\n",
    "5. We finally return the network with new weights and biases, the updated optimizer and the value of the loss function at the current optimization step\n",
    "6. We haved added the \"@eqx.filter_jit\" line of code before the \"make_step\" function to speed up the code - this acts as a decorator to the make step function resulting in the jit compilation of this function\n",
    "7. We will discuss some basic examples of decorators in Python followed by some notes on jit compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e01ae-4d25-4647-829d-bfcbf10c4ebb",
   "metadata": {},
   "source": [
    "## Example of a simple decorator in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8020a0d3-6ba4-4c9e-a2e1-f369f680302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_decorator(func):\n",
    "    def wrapper():\n",
    "        print(\"Something is happening before the function is called.\")\n",
    "        func()\n",
    "        print(\"Something is happening after the function is called.\")\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa31b3e7-7223-4e6a-a9ac-24040a86bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def say_hello():\n",
    "    print(\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a421eb7-a951-4243-8721-16e6d5294ab8",
   "metadata": {},
   "source": [
    "## Execute the \"say_hello\" function without any decorator applied to the function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e154564-075a-420d-a882-294ca0f275e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "say_hello()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad127f3d-b3a5-4260-af30-9fc859f5bef3",
   "metadata": {},
   "source": [
    "## Define a new \"say_hello_with_dec\" function and apply the \"my_decorator\" decorator to this function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6399419c-30ea-4ff0-8bdd-5599d043376d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@my_decorator\n",
    "def say_hello_with_dec():\n",
    "    print(\"Hello!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9b8df9-fd3c-4794-9ab1-2f2405d013d5",
   "metadata": {},
   "source": [
    "## Execute the new function where a decorator has been applied to the definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4841db-4bf0-415c-8705-dc630485b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "say_hello_with_dec()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc92dd13-9fde-4f49-9d85-7c86890e8d96",
   "metadata": {},
   "source": [
    "## Notes on the decorator example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453ec72b-ffad-4b54-8d39-816aeeb21d10",
   "metadata": {},
   "source": [
    "1. We see from the simple decorator example that decorators extend and modify the behavior of functions\n",
    "2. In our example the decorator function took a function \"func\" as input, passed it to a another function called \"wrapper\" which extended the behavior of \"func\" and finally, the decorator returned \"wrapper\" -> the extended version of \"func\"\n",
    "3. The typical syntax is to use the \"@\" symbol followed by the decorator name before defining the function to which we wish to apply the decorator\n",
    "4. Here we apply the decorator \"my decorator\" via the syntax \"@my_decorator\" to the function \"say_hello_with_dec\" before the line that defines this function with the keyword \"def\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c4320e-34a9-45d2-af3f-53bcbc5a8698",
   "metadata": {},
   "source": [
    "## Notes on jit compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d41b892-2279-4771-a989-4c782a350073",
   "metadata": {},
   "source": [
    "1. Jax's just in time (jit) compilation accelerates speed of computation\n",
    "2. The jit compiler in JAX uses XLA (Accelerated Linear Algebra)\n",
    "3. The \"filter\" part is again used to \"filter\" out the weights and biases of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd14a06c-b3c8-4fdb-a983-68010084c9fa",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23b7e0-09fe-40b6-8c34-c97a6e59b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model, opt_state, loss = make_step(model, opt_state, x_samples, y_samples)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, loss: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6077cab3-64a9-41b3-ab16-59691c0a05ad",
   "metadata": {},
   "source": [
    "## Notes on Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1592bc-0df1-4e40-9e76-c8480fd1cb24",
   "metadata": {},
   "source": [
    "1. We simply perform the single optimization step iteratively in a loop via the make step function\n",
    "2. The loss values are extracted so that the loss history can be analyzed later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d195861-1b2b-4dc1-ba45-8381f649a7d3",
   "metadata": {},
   "source": [
    "## Plot loss history on log scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7531b39c-4f5b-4915-bc5c-14dc159bf326",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(loss_history)\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"log loss\")\n",
    "plt.title(\"loss history on log scale\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f2b51-328f-4f29-9a9a-cb49490c08e0",
   "metadata": {},
   "source": [
    "## Generate final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e9df4f-803b-4510-91f5-bd9cf7df1271",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = jax.vmap(model)(x_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f0f831-f697-4622-ba28-bb0c1a615cd2",
   "metadata": {},
   "source": [
    "## Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6625943e-dd23-4e6d-b3c6-74d3b7cab3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(x_samples, y_samples, label=\"toy data\")\n",
    "plt.plot(x_samples, final_pred, 'r', label=\"final prediction\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Sine wave regression in JAX with Equinox and Optax using simple MLP\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
